{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSY59y8Eu_8Z"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mariyamuneeb/ssl_wordspotting/blob/main/VariationalAE.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAvHGaA63gPE"
      },
      "source": [
        "## Installations, Imports, Plotting Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj2iCMvuNyYO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mariyamuneeb/ssl_wordspotting\n",
        "!pip -qqq install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:15:56.923475Z",
          "start_time": "2023-01-22T13:15:53.952995Z"
        },
        "id": "AQcW98vJ8B_Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # this module is useful to work with numerical arrays\n",
        "import pandas as pd \n",
        "import random \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import numpy as np\n",
        "wandb.login(key=\"76fdad476f01ca03a4b43a03616920f905a25488\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "GDRIVE_ROOT = '/content/drive/MyDrive/Datasets'"
      ],
      "metadata": {
        "id": "OnWiTukbwfud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Functions"
      ],
      "metadata": {
        "id": "dzh3U9zxwlhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plotting Few Samples\n",
        "def plot_samples(dataset,num_samples):\n",
        "    random_imgs = dataset.get_random_samples(num_samples)\n",
        "    _, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
        "    axs = axs.flatten()\n",
        "    for img, ax in zip(random_imgs, axs):\n",
        "        ax.imshow(img)\n",
        "        ax.title.set_text(f'Image Shape {img.size},{img.mode}')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "## Plotting Samples During Training\n",
        "def plot_ae_custom_ds_outputs(encoder,decoder,test_dataset,n=10):\n",
        "    wandb_imgs = list()\n",
        "    wandb_rec_imgs = list()\n",
        "    my_table = wandb.Table(columns=[\"Original\", \"Reconstruction\"])\n",
        "    plt.figure(figsize=(16,4.5))   \n",
        "    for i in range(n):\n",
        "      ax = plt.subplot(2,n,i+1)\n",
        "      img = test_dataset[i][0].unsqueeze(0).to(device)\n",
        "      encoder.eval()\n",
        "      decoder.eval()\n",
        "      with torch.no_grad():\n",
        "         rec_img  = decoder(encoder(img))\n",
        "      plt.imshow(img.cpu().squeeze().numpy().T, cmap='gist_gray') # for MNIST remove the transpose\n",
        "    #   plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray') # for MNIST remove the transpose\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "        ax.set_title('Original images')\n",
        "      ax = plt.subplot(2, n, i + 1 + n)\n",
        "      plt.imshow(rec_img.cpu().squeeze().numpy().T, cmap='gist_gray')  #for MNIST remove the transpose\n",
        "    #   plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  #for MNIST remove the transpose\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "         ax.set_title('Reconstructed images')\n",
        "      my_table.add_data( wandb.Image(img.cpu()), wandb.Image(rec_img.cpu()))\n",
        "      wandb_imgs.append(img.cpu())\n",
        "      wandb_rec_imgs.append(rec_img.cpu())\n",
        "    plt.show()   "
      ],
      "metadata": {
        "id": "iRjlwfELwebm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yquBb80ips77"
      },
      "source": [
        "# Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9OReq7bDA7"
      },
      "source": [
        "## Encoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:48.486303Z",
          "start_time": "2023-01-22T13:18:48.466511Z"
        },
        "id": "3mtJjd4N9JQj"
      },
      "outputs": [],
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "    def __init__(self, num_input_channels,\n",
        "                 base_channel_size,\n",
        "                 latent_dims,\n",
        "                 ):\n",
        "        super(VariationalEncoder, self).__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.conv1 = nn.Conv2d(num_input_channels, c_hid, kernel_size = 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(c_hid, 2*c_hid,  kernel_size =3, stride=2, padding=1)\n",
        "        # self.batch2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(2*c_hid, 2*2*c_hid, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(2*2*c_hid, 2*2*2*c_hid, kernel_size=3, stride=2,padding=1)\n",
        "        self.conv5 = nn.Conv2d(2*2*2*c_hid, 2*2*2*2*c_hid, kernel_size=3, padding=1, stride=2)\n",
        "        self.linear1 = nn.Linear(4*4*16*c_hid, 4*2*c_hid)\n",
        "        self.linear2 = nn.Linear(4*2*c_hid, latent_dims)\n",
        "        self.linear3 = nn.Linear(4*2*c_hid, latent_dims)\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        mu =  self.linear2(x)\n",
        "        sigma = torch.exp(self.linear3(x))\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "        return z\n",
        "        # return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCXp_v2xLwZm"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(8,1,128,128)\n",
        "print(x.shape)\n",
        "enc = VariationalEncoder(1, 32,512)\n",
        "z = enc(x)\n",
        "print(z.shape)\n",
        "# z2 = z.reshape(z.shape[0], -1, 4, 4)\n",
        "# print(z2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu--vnwPbJX2"
      },
      "source": [
        "## Decoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:49.970641Z",
          "start_time": "2023-01-22T13:18:49.956157Z"
        },
        "id": "aoZpRCCSbAEm"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_input_channels,\n",
        "                 base_channel_size,\n",
        "                 latent_dims):\n",
        "        super().__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(latent_dims, 4*2*c_hid),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(4*2*c_hid,4*4*16*c_hid),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*2*2*2*c_hid, 2*2*2*c_hid,  kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            # nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(2*2*2*c_hid, 2*2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(2*2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            # nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        # x = self.unflatten(x)\n",
        "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
        "        x = self.decoder_conv(x)\n",
        "        # x = torch.tanh(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzRBjfMtROO3"
      },
      "outputs": [],
      "source": [
        "dec = Decoder(3,32,512)\n",
        "z = dec(z)\n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zm1_ZtV5GrL"
      },
      "source": [
        "## Variational AE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:52.396762Z",
          "start_time": "2023-01-22T13:18:52.387668Z"
        },
        "id": "DH_ulmfO5LTZ"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, num_channels,base_channel_size,latent_dim):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = VariationalEncoder(num_channels,base_channel_size,latent_dim)\n",
        "        self.decoder = Decoder(num_channels,base_channel_size,latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu6SN0-j47Qu"
      },
      "source": [
        "class that merges the encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIt9xMD66mvi"
      },
      "source": [
        "Initialize the VariationalAutoencoder class, the optimizer, and the device to use the GPU in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXW0hTda3FK3"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mF1mk1f7BS7"
      },
      "source": [
        "Functions to train and evaluate the Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:54.420204Z",
          "start_time": "2023-01-22T13:18:54.409295Z"
        },
        "id": "hymyEk5X6yRe"
      },
      "outputs": [],
      "source": [
        "### Training function\n",
        "def train_epoch(vae, device, dataloader, optimizer):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    vae.train()\n",
        "    train_loss = 0.0\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for x, _ in dataloader: \n",
        "        # Move tensor to the proper device\n",
        "        x = x.to(device)\n",
        "        x_hat = vae(x)\n",
        "        # Evaluate loss\n",
        "        loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        # print('\\t partial train loss (single batch): %f' % (loss.item()))\n",
        "        train_loss+=loss.item()\n",
        "    train_loss_ave = train_loss / len(dataloader.dataset)\n",
        "    wandb.log({\"train_loss\": train_loss_ave})\n",
        "    return train_loss_ave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bkw2rl23JUV"
      },
      "source": [
        "## Testing Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVs8GvW8UJj"
      },
      "source": [
        "The loss is composed of two terms. The reconstruction term is the sum of the squared differences between the input and its reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:55.363586Z",
          "start_time": "2023-01-22T13:18:55.354021Z"
        },
        "id": "oYW18qc78ROm"
      },
      "outputs": [],
      "source": [
        "### Testing function\n",
        "def test_epoch(vae, device, dataloader):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    vae.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        for x, _ in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            x = x.to(device)\n",
        "            # Encode data\n",
        "            encoded_data = vae.encoder(x)\n",
        "            # Decode data\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
        "            val_loss += loss.item()\n",
        "    val_loss_ave = val_loss / len(dataloader.dataset)\n",
        "    wandb.log({\"val_loss\": val_loss_ave})    \n",
        "    return val_loss_ave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR3UhqAA3LbP"
      },
      "source": [
        "## Plotting Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbuKaVQC8miu"
      },
      "source": [
        "\n",
        "The input and its corresponding reconstruction in each epoch during the training of the VAE model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:57.063103Z",
          "start_time": "2023-01-22T13:18:57.043128Z"
        },
        "id": "L0_RaOyO8l5z"
      },
      "outputs": [],
      "source": [
        "def plot_ae_outputs(encoder,decoder,n=10):\n",
        "    wandb_imgs = list()\n",
        "    wandb_rec_imgs = list()\n",
        "    my_table = wandb.Table(columns=[\"Original\", \"Reconstruction\"])\n",
        "    plt.figure(figsize=(16,4.5))\n",
        "    targets = np.array(test_dataset.targets) # for MNIST change this to test_dataset.targets.numpy()\n",
        "    # targets =  test_dataset.targets.numpy()\n",
        "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
        "    for i in range(n):\n",
        "      ax = plt.subplot(2,n,i+1)\n",
        "      img = test_dataset[t_idx[i]][0].unsqueeze(0).to(device)\n",
        "      encoder.eval()\n",
        "      decoder.eval()\n",
        "      with torch.no_grad():\n",
        "         rec_img  = decoder(encoder(img))\n",
        "      plt.imshow(img.cpu().squeeze().numpy().T, cmap='gist_gray') # for MNIST remove the transpose\n",
        "    #   plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray') # for MNIST remove the transpose\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "        ax.set_title('Original images')\n",
        "      ax = plt.subplot(2, n, i + 1 + n)\n",
        "      plt.imshow(rec_img.cpu().squeeze().numpy().T, cmap='gist_gray')  #for MNIST remove the transpose\n",
        "    #   plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  #for MNIST remove the transpose\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "         ax.set_title('Reconstructed images')\n",
        "      my_table.add_data( wandb.Image(img.cpu()), wandb.Image(rec_img.cpu()))\n",
        "      wandb_imgs.append(img.cpu())\n",
        "      wandb_rec_imgs.append(rec_img.cpu())\n",
        "    plt.show()\n",
        "    # my_table = wandb.Table()\n",
        "    \n",
        "    \n",
        "\n",
        "    # my_table.add_column(\"Original\", wandb_imgs)\n",
        "    # my_table.add_column(\"Reconstruction\", wandb_rec_imgs)\n",
        "\n",
        "# Log your Table to W&B\n",
        "    wandb.log({\"vae_reconstrunction_cifar10\": my_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oo4UDu_qV5O"
      },
      "source": [
        "# Standard Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nyjQvgh22_S"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrSkWjm1a1RJ"
      },
      "outputs": [],
      "source": [
        "num_channels = 3\n",
        "base_channel_size=32\n",
        "lr = 10e-2\n",
        "latent_dim = 384\n",
        "epochs = 300\n",
        "plot_freq = 10\n",
        "\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"SSL\", \n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"VAE\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"architecture\": \"CNN\",\n",
        "      \"dataset\": \"CIFAR-10\",\n",
        "      \"epochs\": epochs,\n",
        "      \"latent_dim\":latent_dim\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6FoQuN853ip"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# d = 4\n",
        "\n",
        "vae = VariationalAutoencoder(latent_dims=4)\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "vae.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J1cgQvs3s-O"
      },
      "source": [
        "## Import/Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGl005fo8RZM"
      },
      "outputs": [],
      "source": [
        "data_dir = 'dataset'\n",
        "# train_dataset = torchvision.datasets.MNIST(data_dir, train = True, download = True)\n",
        "# test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(data_dir,train=True,download=True)\n",
        "test_dataset  = torchvision.datasets.CIFAR10(data_dir,train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brvR2L49xNMW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsgnrzo2KzQA"
      },
      "outputs": [],
      "source": [
        "img = train_dataset[1][0]\n",
        "label = train_dataset[1][1]\n",
        "print(img.mode)\n",
        "print(label)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J81ybJ2Wml-o"
      },
      "outputs": [],
      "source": [
        "img.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SlmWgwP8tWc"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.ToTensor(), ])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJf4FmL-9E7E"
      },
      "outputs": [],
      "source": [
        "train_dataset.transform = train_transform\n",
        "test_dataset.transform = test_transform\n",
        "\n",
        "m=len(train_dataset)\n",
        "\n",
        "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
        "batch_size=256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q074Zmjj9bPG"
      },
      "source": [
        "VariationalAutoencoder class, which combines the Encoder and Decoder classes \n",
        "The encoder and decoder networks contain **three convolutional layers** and **two fully connected layers**. \n",
        "Some batch normal layers are added to have more robust features in the latent space. \n",
        "Differently from the standard autoencoder, the **encoder returns mean and variance matrices** and we use them to obtain the sampled latent vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93a6o1jq3OHs"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwUn0p7S9U3e"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "   train_loss = train_epoch(vae,device,train_loader,optim)\n",
        "   val_loss = test_epoch(vae,device,valid_loader)\n",
        "   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, epochs,train_loss,val_loss))\n",
        "   if epoch%plot_freq==0:\n",
        "       plot_ae_outputs(vae.encoder,vae.decoder,n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m0ufLwUVtrT"
      },
      "outputs": [],
      "source": [
        "break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8qkBjedVngN"
      },
      "outputs": [],
      "source": [
        "train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO2dYAoMoV7N"
      },
      "source": [
        "# VAE on Hand-written Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYQ-39cjHT5o"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZknEdk_r_kqt"
      },
      "source": [
        "### Connect to GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5jN5BN_-cBw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Datasets/tif'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZidUqcI_sgL"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MysQxraoeTzI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import math\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from statistics import mean\n",
        "from math import floor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOF8HLNQ_zee"
      },
      "source": [
        "### Custom Dataset Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hry8wDvv__tJ"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_paths,transform=None):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.img_paths = img_paths\n",
        "        self.transform = transform       \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.img_paths[idx]\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image,'1'\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        dummy_targets = ['null']*len(self.img_paths)\n",
        "        return dummy_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhz8RoA9AC4W"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slV_LJ0-AIaY"
      },
      "outputs": [],
      "source": [
        "images_paths = [os.path.join(PATH,i) for i in os.listdir(PATH)]\n",
        "split = 0.85\n",
        "train_idx = math.floor(split*len(images_paths))\n",
        "train_images = images_paths[:train_idx]\n",
        "test_images = images_paths[train_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqLz_mGNBRn9"
      },
      "source": [
        "### Plotting Few Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BreM58fbBO-D"
      },
      "outputs": [],
      "source": [
        "random_imgs = random.sample(train_images, 9)\n",
        "random_imgs = [Image.open(i) for i in random_imgs]\n",
        "_, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
        "axs = axs.flatten()\n",
        "for img, ax in zip(random_imgs, axs):\n",
        "    ax.imshow(img)\n",
        "    ax.title.set_text(f'Image Shape {img.size},{img.mode}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB-qzqetELqW"
      },
      "source": [
        "### Finding Ave Image Dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-jxGkyXESt8"
      },
      "outputs": [],
      "source": [
        "h_list = list()\n",
        "w_list = list()\n",
        "\n",
        "for p in train_images:\n",
        "    h_list.append(Image.open(p).size[1])\n",
        "    w_list.append(Image.open(p).size[0])\n",
        "num_channels = int(Image.open(p).mode)\n",
        "\n",
        "h_ave = floor(mean(h_list))\n",
        "w_ave = floor(mean(w_list))\n",
        "# resize_size = (h_ave,w_ave)\n",
        "resize_size = (128,128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chw7hy3hz95m"
      },
      "source": [
        "### Dataset Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzWS8sYNnxkT"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Resize(resize_size),])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Resize(resize_size),])\n",
        "\n",
        "hw_train_dataset = MyDataset(img_paths=train_images,transform=train_transform)\n",
        "hw_test_dataset = MyDataset(img_paths=test_images,transform=test_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0R17Iq4HIKw"
      },
      "source": [
        "### Dataloader and Batching Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA8qrO3PHdiK"
      },
      "outputs": [],
      "source": [
        "m=len(hw_train_dataset)\n",
        "\n",
        "\n",
        "batch_size=8\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(hw_train_dataset, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(hw_test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG3GD4JO0INN"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:19:08.602612Z",
          "start_time": "2023-01-22T13:19:08.586896Z"
        },
        "id": "oAEo-8l70Ktj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zuJMXv_KK8"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0pqFiZIu_9J"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T11:19:05.358802Z",
          "start_time": "2023-01-22T11:19:05.353833Z"
        },
        "id": "Vw9QQcCC_KK-"
      },
      "outputs": [],
      "source": [
        "base_channel_size = 32\n",
        "lr = 1e-3\n",
        "latent_dim = 512\n",
        "epochs = 300\n",
        "plot_freq = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COtz1mTVKD1m"
      },
      "outputs": [],
      "source": [
        "hw_train_dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l-sFOl8GXnl"
      },
      "source": [
        "### W&B Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUv8GX5WGW3C"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"SSL\", \n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"VAE\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"architecture\": \"CNN\",\n",
        "      \"dataset\": \"Hand Written Dataset\",\n",
        "      \"lr\":lr,\n",
        "      \"epochs\": epochs,\n",
        "      \"latent_dim\":latent_dim\n",
        "      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3OLrMd_KLC"
      },
      "source": [
        "### Initialize VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frlAmJr1_KLE"
      },
      "source": [
        "Initialize the VariationalAutoencoder class, the optimizer, and the device to use the GPU in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8wALQrK_KLF"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "vae = VariationalAutoencoder(num_channels,base_channel_size,latent_dim)\n",
        "print(vae)\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "vae.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JR2TvqtIStx"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqhaSG1lIWhq"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T11:17:05.010949Z",
          "start_time": "2023-01-22T11:17:05.006278Z"
        },
        "id": "hX_doqxrIVJW"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "   train_loss = train_epoch(vae,device,train_loader,optim)\n",
        "   val_loss = test_epoch(vae,device,valid_loader)\n",
        "   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, epochs,train_loss,val_loss))\n",
        "   if epoch%plot_freq==0:\n",
        "       plot_ae_custom_ds_outputs(vae.encoder,vae.decoder,hw_test_dataset,n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMUgJJWl7ryg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# IAM Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KT02E0-u_9P"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:20:48.913307Z",
          "start_time": "2023-01-22T13:20:48.907008Z"
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4cnqB0xau_9P"
      },
      "outputs": [],
      "source": [
        "from models.dataset_utils import IAMDataset\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:15:45.403362Z",
          "start_time": "2023-01-22T13:15:45.296090Z"
        },
        "id": "-2eX4W4Pu_9Q"
      },
      "outputs": [],
      "source": [
        "dataset_root_dir = f'{GDRIVE_ROOT}/IAM_HW/words_full_dataset'\n",
        "train_dir = f'{dataset_root_dir}/words_training'\n",
        "test_dir = f'{dataset_root_dir}/words_test'\n",
        "iam_train_dataset = IAMDataset(train_dir)\n",
        "iam_test_dataset = IAMDataset(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:17:55.194171Z",
          "start_time": "2023-01-22T13:17:55.188510Z"
        },
        "id": "dXvj8NNHu_9Q"
      },
      "outputs": [],
      "source": [
        "batch_size=8\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(iam_train_dataset, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(iam_test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:22:24.621128Z",
          "start_time": "2023-01-22T13:22:22.211674Z"
        },
        "id": "SJJsEdRZu_9R"
      },
      "outputs": [],
      "source": [
        "h_list = list()\n",
        "w_list = list()\n",
        "\n",
        "for p,_ in iam_train_dataset:\n",
        "    h_list.append(p.size[1])\n",
        "    w_list.append(p.size[0])\n",
        "num_channels = 3\n",
        "\n",
        "h_ave = floor(mean(h_list))\n",
        "w_ave = floor(mean(w_list))\n",
        "# resize_size = (h_ave,w_ave)\n",
        "resize_size = (128,128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:21.744306Z",
          "start_time": "2023-01-22T13:18:20.080491Z"
        },
        "id": "QK2Exc4Du_9R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edJTS35pu_9T"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujBtQDmFu_9T"
      },
      "source": [
        "### HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:31.486979Z",
          "start_time": "2023-01-22T13:18:31.481712Z"
        },
        "id": "UA2wtpKMu_9T"
      },
      "outputs": [],
      "source": [
        "base_channel_size = 32\n",
        "lr = 1e-3\n",
        "latent_dim = 512\n",
        "epochs = 300\n",
        "plot_freq = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhI4y6dmu_9U"
      },
      "source": [
        "### W&B Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:18:41.276449Z",
          "start_time": "2023-01-22T13:18:33.880504Z"
        },
        "id": "3zqft9glu_9V"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"SSL\", \n",
        "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "      name=f\"VAE\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"architecture\": \"CNN\",\n",
        "      \"dataset\": \"Hand Written Dataset\",\n",
        "      \"lr\":lr,\n",
        "      \"epochs\": epochs,\n",
        "      \"latent_dim\":latent_dim\n",
        "      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JujgaDLmu_9V"
      },
      "source": [
        "### Initialize VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:22:34.287662Z",
          "start_time": "2023-01-22T13:22:34.155825Z"
        },
        "id": "bgYkvruyu_9W"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "vae = VariationalAutoencoder(num_channels,base_channel_size,latent_dim)\n",
        "print(vae)\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "vae.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0kCI6hpu_9W"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVFUGu9du_9W"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-01-22T13:22:41.894928Z",
          "start_time": "2023-01-22T13:22:41.761712Z"
        },
        "id": "RlD_DSoJu_9W"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "   train_loss = train_epoch(vae,device,train_loader,optim)\n",
        "   val_loss = test_epoch(vae,device,valid_loader)\n",
        "   print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, epochs,train_loss,val_loss))\n",
        "   if epoch%plot_freq==0:\n",
        "       plot_ae_custom_ds_outputs(vae.encoder,vae.decoder,hw_test_dataset,n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNcUeZxTu_9Y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zAvHGaA63gPE",
        "93a6o1jq3OHs"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}